---
title: Cloning in your classroom
description: "Digital twins could transform the way we teach English."
authors: ["Max Bruges"]
date: 2025-05-04
draft: false
extra:
  icon: ðŸ§¬
---

![HERO Dolly the Sheep](/images/dolly.webp)
*[Dolly the sheep](https://en.wikipedia.org/wiki/Dolly_(sheep)) (left).*

"But what's the _right_ answer, sir?"

I'd barely been teaching a week before being brought to a dead stop, mid-lesson. I'd done all the right things: pre-taught the vocabulary, broken down the context, [painstakingly annotated](@/blog/definitions.md) the iambs and trochees. But that had all been [vanity](https://www.biblegateway.com/passage/?search=ecclesiastes%201&version=KJV).

Rafael wanted an *answer*. And he wanted the right one.

No amount of "well, one could argue..." or "a possible interpretation might be..." or any critical expostulation would satisfy him. If Maths and Science and French and ICT could answer their questions with a tick or a cross, why couldn't English?

## Spinning the flywheel

Rafael was right, of course. This is the problem with English. Our answers are too fuzzy. There isn't one, immutable, correct response to _'How does Shakespeare portray [the protagonist](@/learn/textbook/macbeth.md) as a tragic hero?'_ - and Harold Bloom will rise from his grave and throttle you if you even suggest it.

![Harold Bloom in his study, aside](/images/harold-bloom.webp)
*Not angry, just disappointed.*

But that is also the _point_: we do not teach knowledge. We teach the encoding and decoding of language, in all its fuzzy nuance.

This makes the essential iterative loop of learning - `answer -->` `assess -->` `improve -->` `re-answer` - a slow, manual process. Every answer from a student in English needs to be weighed on [Thoth's](https://en.wikipedia.org/wiki/Thoth) scales of criticality, and only the teacher in the room is fully qualified to make that judgment. We look in envy at our STEM colleagues with their binary marking and autonomous testing; for every sentence we evaluate for the nuance of language usage, they can mark a dozen sums `right` or `wrong`. The maths student receives an order of magnitude more indicators of success and correctness that the English student does, even before accounting for the ease of self-assessment.

The bottleneck to learning in Arts and Humanities is the teacher's capacity to assess and feedback. But that could be about to change.

## Fuzzy bots, firm answers

Above everything, LLMs excel at fuzzily assessing language, which makes them the perfect candidate for 'cloning' an English teacher.

![aside](/images/twiins.webp)
*Yes I know they aren't [clones](https://en.wikipedia.org/wiki/Twins_(1988_film)).*

They may [hallucinate facts](@/blog/BanGoogle.md) or forget data, but their ability to read and extrapolate from written text is dependably good. Ultimately, that's what we need in English. We learn it the same way the LLM does: ingesting as much [good quality writing](@/blog/PowerOfReading.md) as possible so that we can produce our own, mimicking styles and structures whilst changing the content.

The classroom of tomorrow can leverage this technology to finally widen that bottleneck; a [digital twin](https://en.wikipedia.org/wiki/Digital_twin) of the teacher to serve as a low-stakes feedback machine.

They don't need to be perfect, only 'good enough' to give an indication of if the student is the right direction: finally giving English some semblance of the STEM binary; though rather than `correct/incorrect`, we can ask for `better/worse`.

## Cautious optimisation

But tread softly, and never mistake eloquence for accuracy. The key distinction here is between *evaluation* and *feedback*. The bots are rubbish at the former: too easily swayed by context and weighting to reliably give accurate marks and levels for written work.

In my own rudimentary experiements, the bots showed the sort of capricious inconsistency that would get an examiner booted from a marking pool in minutes, with grading that varied wildly even from paragraph to paragraph. Running the same exam script through the AI three times would produce three distinctly different results. There have been [some promising steps in recent months](https://schoolsweek.co.uk/using-ai-to-judge-writing-could-revolutionise-assessment-trial/) to improve on this, but at this stage the tech is simply too inconsistent to take on the quantitative aspect of marking. We may get there with a big dollop of fine-tuning and RAG-ing, but for now the auto-marking bot remains but a twinkle in every GCSE assessor's eye.

![A Vogon receiving criticism on his poetry](/images/vogon-poetry.webp)
*"The candidate has demonstrated an ambitious use of vocabulary, commensurate with AO3 Level 4."*

Feedback, on the other hand, is ideal. Load up the AI's context window with a script of pre-written responses ('Vary your sentence structures', 'Consider your paragraphing', 'Incorporate more ambitious vocabulary', etc.) and they can _usually_ be counted on to pick the right one for the right text. And for obvious reasons: it's doing the job it was made for, to extrapolate text. Assessment frameworks may come and go, marking criteria may change with phases of the moon, but good, broad feedback is forever.

For low-stakes assessment, where the _practice_ rather than the _outcome_ is what matters, this has the potential to work brilliantly. A quick nudge in the right direction can keep momentum up and spin the learning flywheel; nudges that you don't always have the time or space to deliver.

In truth, the only way to know is to do. Run some student work through ChatGPT, with a strict prompt limiting them to your pre-written comments. See what it comes up with. Feed in some more tailored comment templates. Tweak, repeat, iterate.

It may not be classroom-ready today, but you may have a feedback clone to help you tomorrow.
