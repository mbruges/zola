---
title: Cloning in your classroom
description: "Digital twins can transform the way we teach English."
authors: ["Max Bruges"]
date: 2025-05-04
draft: false
extra:
  icon: ðŸ‘¯
---

![bg](/images/clone-army.webp)
*Begun, the LLM wars have.*

"But what's the _right_ answer, sir?"

I'd barely been teaching a week before being brought to a crashing halt. I'd done all the right things: pre-taught the vocabulary, broken down the context, painstakingly annotated the iambs and trochees. But that had all been [vanity](https://www.biblegateway.com/passage/?search=ecclesiastes%201&version=KJV).

Rafael wanted an answer. And he wanted the right one. And no amount of "well, one could argue" or "a possible interpretation might be" would satisfy him. If Maths and Science and French and ICT could answer their questions with a tick or a cross, why couldn't English?

## Spinning the flywheel

Rafael was right, of course. This is the problem with English (and History, and all the 'essay subjects'). Our answers are too _fuzzy_. There isn't one correct answer to 'How does Shakespeare portray the protagonist as a tragic hero?' - and Harold Bloom will rise from his grave and throttle you if you even suggest it.

![Harold Bloom in his study, aside](/images/harold-bloom.webp)
*Not angry, just disappointed.*

But that is also the _point_: we do not teach knowledge. We teach the encoding and decoding of language, in all its fuzzy nuance.

This makes the essential iterative loop of learning - `answer --> assess --> improve --> re-answer` - a slow, manual process. Every answer from a student in English needs to be weighed on [Thoth's](https://en.wikipedia.org/wiki/Thoth) scales of criticality, and only the teacher in the room is fully qualified to make that judgment. We can look in envy at our STEM colleagues with their binary, autonomous testing; for every sentence we evaluate for language usage, they can mark a dozen sums right or wrong. The maths student receives an order of magnitude more indicators of success and correctness that the English student does, even before accounting for the ease of self-assessment.

The bottleneck to learning in Arts and Humanities is the teacher's capacity to assess and feedback. But that could be about to change.

## Fuzzy bots, firm answers

Above everything, LLMs excel at fuzzily assessing language, which makes them the perfect candidate for 'cloning' an English teacher.

![aside](/images/twiins.webp)
*Yes I know they aren't clones.*

They may hallucinate facts or forget data, but their ability to read and extrapolate from written text is dependably good. Ultimately, that's what we need in English. We learn it the same way the LLM does: ingesting as much good quality writing as possible so that we can produce our own, mimicking styles and structures whilst changing the content.

The classroom of tomorrow can leverage this technology to finally widen that bottleneck; a [digital twin](https://en.wikipedia.org/wiki/Digital_twin) of the teacher to serve as a low-stakes feedback machine.

They don't need to be perfect, only 'good enough' to give an indication of if the student is the right direction: finally giving English some semblance of the STEM binary; though rather than `correct/incorrect`, we can ask for `better/worse`.

## Cautious optimisation

But tread softly, and never mistake eloquence for accuracy. The key distinction here is between *evaluation* and *feedback*. The bots are rubbish at the former: too easily swayed by context and weighting to reliably give accurate marks and levels for written work.

My own experimenting showed the sort of capricious inconsistency that would get an examiner booted from a marking pool in minutes, grading that varied wildly even from paragraph to paragraph. As much as it may be the fantasy of every GCSE teacher, the bot can't do the marking for us just yet.

![A Vogon receiving criticism on his poetry](/images/vogon-poetry.webp)
*"The candidate has demonstrated an effective use of vocabulary."*

Feedback, on the other hand, is ideal. Load up the AI's context window with a script of pre-written responses ('Vary your sentence structures', 'Consider your paragraphing', 'Incorporate more ambitious vocabulary', etc.) and they can _usually_ be counted on to pick the right one for the right text. And for obvious reasons: it's doing the job it was made for, to extrapolate text. Assessment frameworks may come and go, marking criteria may change with phases of the moon, but good broad feedback is forever.

For low-stakes assessment, where the _practice_ rather than the _outcome_ is what matters, this has the potential to work brilliantly.

In truth, the only way to know is to do. Run some student work through ChatGPT, with a strict prompt limiting them to your written comments. See what it comes up with. Tweak, repeat, iterate. Because if the alternative is work that goes unmarked and unremarked, then this is worth a try.
